{"metadata":{"language_info":{"name":"python","version":"3.7.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pattern","metadata":{"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pattern\n  Downloading Pattern-3.6.0.tar.gz (22.2 MB)\n\u001b[K     |████████████████████████████████| 22.2 MB 18.2 MB/s eta 0:00:01\n\u001b[?25hCollecting future\n  Downloading future-0.18.2.tar.gz (829 kB)\n\u001b[K     |████████████████████████████████| 829 kB 42.0 MB/s eta 0:00:01\n\u001b[?25hCollecting backports.csv\n  Downloading backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\nCollecting mysqlclient\n  Downloading mysqlclient-2.0.3.tar.gz (88 kB)\n\u001b[K     |████████████████████████████████| 88 kB 3.3 MB/s  eta 0:00:01\n\u001b[?25hCollecting beautifulsoup4\n  Downloading beautifulsoup4-4.9.3-py3-none-any.whl (115 kB)\n\u001b[K     |████████████████████████████████| 115 kB 57.9 MB/s eta 0:00:01\n\u001b[?25hCollecting lxml\n  Downloading lxml-4.6.3-cp37-cp37m-manylinux1_x86_64.whl (5.5 MB)\n\u001b[K     |████████████████████████████████| 5.5 MB 42.8 MB/s eta 0:00:01\n\u001b[?25hCollecting feedparser\n  Downloading feedparser-6.0.2-py3-none-any.whl (80 kB)\n\u001b[K     |████████████████████████████████| 80 kB 8.6 MB/s  eta 0:00:01\n\u001b[?25hCollecting pdfminer.six\n  Downloading pdfminer.six-20201018-py3-none-any.whl (5.6 MB)\n\u001b[K     |████████████████████████████████| 5.6 MB 58.6 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy in /srv/conda/envs/notebook/lib/python3.7/site-packages (from pattern) (1.20.1)\nRequirement already satisfied: scipy in /srv/conda/envs/notebook/lib/python3.7/site-packages (from pattern) (1.6.1)\nCollecting nltk\n  Downloading nltk-3.6.2-py3-none-any.whl (1.5 MB)\n\u001b[K     |████████████████████████████████| 1.5 MB 62.5 MB/s eta 0:00:01\n\u001b[?25hCollecting python-docx\n  Downloading python-docx-0.8.10.tar.gz (5.5 MB)\n\u001b[K     |████████████████████████████████| 5.5 MB 47.6 MB/s eta 0:00:01     |██████████▎                     | 1.8 MB 47.6 MB/s eta 0:00:01\n\u001b[?25hCollecting cherrypy\n  Downloading CherryPy-18.6.0-py2.py3-none-any.whl (419 kB)\n\u001b[K     |████████████████████████████████| 419 kB 58.8 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: requests in /srv/conda/envs/notebook/lib/python3.7/site-packages (from pattern) (2.25.1)\nCollecting soupsieve>1.2\n  Downloading soupsieve-2.2.1-py3-none-any.whl (33 kB)\nCollecting jaraco.collections\n  Downloading jaraco.collections-3.3.0-py3-none-any.whl (9.9 kB)\nCollecting portend>=2.1.1\n  Downloading portend-2.7.1-py3-none-any.whl (5.3 kB)\nCollecting more-itertools\n  Downloading more_itertools-8.7.0-py3-none-any.whl (48 kB)\n\u001b[K     |████████████████████████████████| 48 kB 2.6 MB/s  eta 0:00:01\n\u001b[?25hCollecting zc.lockfile\n  Downloading zc.lockfile-2.0-py2.py3-none-any.whl (9.7 kB)\nCollecting cheroot>=8.2.1\n  Downloading cheroot-8.5.2-py2.py3-none-any.whl (97 kB)\n\u001b[K     |████████████████████████████████| 97 kB 1.7 MB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: six>=1.11.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from cheroot>=8.2.1->cherrypy->pattern) (1.15.0)\nCollecting jaraco.functools\n  Downloading jaraco.functools-3.3.0-py3-none-any.whl (6.8 kB)\nCollecting tempora>=1.8\n  Downloading tempora-4.0.2-py3-none-any.whl (14 kB)\nRequirement already satisfied: pytz in /srv/conda/envs/notebook/lib/python3.7/site-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern) (2021.1)\nCollecting sgmllib3k\n  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\nCollecting jaraco.text\n  Downloading jaraco.text-3.5.0-py3-none-any.whl (8.1 kB)\nCollecting jaraco.classes\n  Downloading jaraco.classes-3.2.1-py3-none-any.whl (5.6 kB)\nRequirement already satisfied: joblib in /srv/conda/envs/notebook/lib/python3.7/site-packages (from nltk->pattern) (1.0.1)\nRequirement already satisfied: click in /srv/conda/envs/notebook/lib/python3.7/site-packages (from nltk->pattern) (7.1.2)\nCollecting tqdm\n  Downloading tqdm-4.60.0-py2.py3-none-any.whl (75 kB)\n\u001b[K     |████████████████████████████████| 75 kB 2.3 MB/s  eta 0:00:01\n\u001b[?25hCollecting regex\n  Downloading regex-2021.4.4-cp37-cp37m-manylinux2014_x86_64.whl (720 kB)\n\u001b[K     |████████████████████████████████| 720 kB 45.8 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: sortedcontainers in /srv/conda/envs/notebook/lib/python3.7/site-packages (from pdfminer.six->pattern) (2.3.0)\nRequirement already satisfied: cryptography in /srv/conda/envs/notebook/lib/python3.7/site-packages (from pdfminer.six->pattern) (3.4.4)\nRequirement already satisfied: chardet in /srv/conda/envs/notebook/lib/python3.7/site-packages (from pdfminer.six->pattern) (4.0.0)\nRequirement already satisfied: cffi>=1.12 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from cryptography->pdfminer.six->pattern) (1.14.4)\nRequirement already satisfied: pycparser in /srv/conda/envs/notebook/lib/python3.7/site-packages (from cffi>=1.12->cryptography->pdfminer.six->pattern) (2.20)\nRequirement already satisfied: certifi>=2017.4.17 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests->pattern) (2020.12.5)\nRequirement already satisfied: idna<3,>=2.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests->pattern) (2.10)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests->pattern) (1.26.3)\nRequirement already satisfied: setuptools in /srv/conda/envs/notebook/lib/python3.7/site-packages (from zc.lockfile->cherrypy->pattern) (49.6.0.post20210108)\nBuilding wheels for collected packages: pattern, future, mysqlclient, python-docx, sgmllib3k\n  Building wheel for pattern (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pattern: filename=Pattern-3.6-py3-none-any.whl size=22332723 sha256=46173c3db1caceb2abb8ce4be41843fbdf5d20b82dc939a7d3edaafbb6883ab9\n  Stored in directory: /home/jovyan/.cache/pip/wheels/8d/1f/4e/9b67afd2430d55dee90bd57618dd7d899f1323e5852c465682\n  Building wheel for future (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491059 sha256=62ad1183a923585c4353ecdf17f027eb769ac2cfaa252f878093b7609118da99\n  Stored in directory: /home/jovyan/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n  Building wheel for mysqlclient (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for mysqlclient: filename=mysqlclient-2.0.3-cp37-cp37m-linux_x86_64.whl size=59681 sha256=0bc38614e38246cf3e655d14192fe9d46847d31fddda499a2fc9fd41a54ceb4f\n  Stored in directory: /home/jovyan/.cache/pip/wheels/79/1c/f8/11fafab45fe6696eea63794a5d747b9c6b54990ac6f1885fb7\n  Building wheel for python-docx (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for python-docx: filename=python_docx-0.8.10-py3-none-any.whl size=184489 sha256=27276fcf7a5d5d86c8e431766e8db48c36c3511af4aa9238eab2a06bb32d8458\n  Stored in directory: /home/jovyan/.cache/pip/wheels/75/c6/69/05491f32dc052cd70476b65f5bf7082a9b274045f6b001b821\n  Building wheel for sgmllib3k (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6065 sha256=4e9eb5d3d02c21642ea0cc775493d1e2b7150af916add6daa7670dec9af38080\n  Stored in directory: /home/jovyan/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\nSuccessfully built pattern future mysqlclient python-docx sgmllib3k\nInstalling collected packages: more-itertools, jaraco.functools, tempora, jaraco.text, jaraco.classes, zc.lockfile, tqdm, soupsieve, sgmllib3k, regex, portend, lxml, jaraco.collections, cheroot, python-docx, pdfminer.six, nltk, mysqlclient, future, feedparser, cherrypy, beautifulsoup4, backports.csv, pattern\nSuccessfully installed backports.csv-1.0.7 beautifulsoup4-4.9.3 cheroot-8.5.2 cherrypy-18.6.0 feedparser-6.0.2 future-0.18.2 jaraco.classes-3.2.1 jaraco.collections-3.3.0 jaraco.functools-3.3.0 jaraco.text-3.5.0 lxml-4.6.3 more-itertools-8.7.0 mysqlclient-2.0.3 nltk-3.6.2 pattern-3.6 pdfminer.six-20201018 portend-2.7.1 python-docx-0.8.10 regex-2021.4.4 sgmllib3k-1.0.0 soupsieve-2.2.1 tempora-4.0.2 tqdm-4.60.0 zc.lockfile-2.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Tokenizing, POS Tagging, and Chunking","metadata":{}},{"cell_type":"code","source":"from pattern.en import parse\nfrom pattern.en import pprint","metadata":{"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/srv/conda/envs/notebook/lib/python3.7/site-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  dtype=np.int):\n/srv/conda/envs/notebook/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n/srv/conda/envs/notebook/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n/srv/conda/envs/notebook/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n/srv/conda/envs/notebook/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n/srv/conda/envs/notebook/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n/srv/conda/envs/notebook/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  eps=np.finfo(np.float).eps, positive=False):\n/srv/conda/envs/notebook/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n/srv/conda/envs/notebook/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n/srv/conda/envs/notebook/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n","output_type":"stream"}]},{"cell_type":"code","source":"#import the en module from the pattern library\n#pprint method to print the output\n#e parse method on the console, you should see the following output\npprint(parse('I drove my car to the hospital yesterday', relations=True, lemmata=True))","metadata":{"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"          WORD   TAG    CHUNK   ROLE   ID     PNP    LEMMA       \n                                                                 \n             I   PRP    NP      SBJ    1      -      i           \n         drove   VBD    VP      -      1      -      drive       \n            my   PRP$   NP      OBJ    1      -      my          \n           car   NN     NP ^    OBJ    1      -      car         \n            to   TO     -       -      -      -      to          \n           the   DT     NP      -      -      -      the         \n      hospital   NN     NP ^    -      -      -      hospital    \n     yesterday   NN     NP ^    -      -      -      yesterday   \n","output_type":"stream"}]},{"cell_type":"markdown","source":"Split() Method","metadata":{}},{"cell_type":"code","source":"from pattern.en import parse\nfrom pattern.en import pprint\n\nprint(parse('I drove my car to the hospital yesterday', relations=True, lemmata=True).split())","metadata":{"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"[[['I', 'PRP', 'B-NP', 'O', 'NP-SBJ-1', 'i'], ['drove', 'VBD', 'B-VP', 'O', 'VP-1', 'drive'], ['my', 'PRP$', 'B-NP', 'O', 'NP-OBJ-1', 'my'], ['car', 'NN', 'I-NP', 'O', 'NP-OBJ-1', 'car'], ['to', 'TO', 'O', 'O', 'O', 'to'], ['the', 'DT', 'B-NP', 'O', 'O', 'the'], ['hospital', 'NN', 'I-NP', 'O', 'O', 'hospital'], ['yesterday', 'NN', 'I-NP', 'O', 'O', 'yesterday']]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Pluralizing and Singularizing the Tokens","metadata":{}},{"cell_type":"code","source":"from pattern.en import pluralize, singularize\n\nprint(pluralize('leaf'))\nprint(singularize('theives'))","metadata":{"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"leaves\ntheife\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Converting Adjective to Comparative and Superlative Degrees","metadata":{}},{"cell_type":"code","source":"from pattern.en import comparative, superlative\n\nprint(comparative('good'))\nprint(superlative('good'))","metadata":{"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"better\nbest\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Finding N-Grams","metadata":{}},{"cell_type":"code","source":"from pattern.en import ngrams\nprint(ngrams(\"He goes to hospital\", n=2))","metadata":{"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"[('He', 'goes'), ('goes', 'to'), ('to', 'hospital')]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Finding Sentiments","metadata":{}},{"cell_type":"code","source":"from pattern.en import sentiment\nprint(sentiment(\"This is an excellent movie to watch. I really love it\"))","metadata":{"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"(0.75, 0.8)\n","output_type":"stream"}]},{"cell_type":"code","source":"The sentence \"This is an excellent movie to watch. I really love it\" has a sentiment of 0.75, which shows that it is highly positive. \nSimilarly, the subjectivity of 0.8 refers to the fact that the sentence is a personal opinion of the user.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking if a Statement is a Fact","metadata":{}},{"cell_type":"code","source":"from pattern.en import parse, Sentence   #we first import the parse method along with the Sentence class\nfrom pattern.en import modality    #import the modality function\n\ntext = \"Paris is the capital of France\"\n\n#The parse method takes text as input and returns a tokenized form of the text, which is then passed to the Sentence class constructor. \n#The modality method takes the Sentence class object and returns the modality of the sentence.\nsent = parse(text, lemmata=True)\nsent = Sentence(sent)\n\nprint(modality(sent))","metadata":{"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"text = \"I think we can complete this task\"\nsent = parse(text, lemmata=True)\nsent = Sentence(sent)\n\nprint(modality(sent))","metadata":{"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"0.25\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Spelling Corrections","metadata":{}},{"cell_type":"code","source":"from pattern.en import suggest\n\nprint(suggest(\"Whitle\"))","metadata":{"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"[('While', 0.6459209419680404), ('White', 0.2968881412952061), ('Title', 0.03280067283431455), ('Whistle', 0.023549201009251473), ('Chile', 0.0008410428931875525)]\n","output_type":"stream"}]},{"cell_type":"code","source":"According to the suggest method, there is a 0.64 probability that the word is \"While\", similarly there is a probability of 0.29 that the word is \"White\", and so on.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pattern.en import suggest\nprint(suggest(\"Fracture\"))#100% chance that the word is spelled correctly.","metadata":{"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"[('Fracture', 1.0)]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Working with Numbers","metadata":{}},{"cell_type":"code","source":"from pattern.en import number, numerals\n\nprint(number(\"one hundred and twenty two\"))\nprint(numerals(256.390, round=2))","metadata":{"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"122\ntwo hundred and fifty-six point thirty-nine\n","output_type":"stream"}]},{"cell_type":"code","source":"In the output, you will see 122 which is the numeric representation of text \"one hundred and twenty-two\". Similarly, \nyou should see \"two hundred and fifty-six point thirty-nine\" which is text representation of the number 256.390.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Quantify","metadata":{}},{"cell_type":"code","source":"from pattern.en import quantify\n\nprint(quantify(['apple', 'apple', 'apple', 'banana', 'banana', 'banana', 'mango', 'mango']))","metadata":{"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"several bananas, several apples and a pair of mangoes\n","output_type":"stream"}]},{"cell_type":"code","source":"from pattern.en import quantify\n\nprint(quantify({'strawberry': 200, 'peach': 15}))\nprint(quantify('orange', amount=1200))","metadata":{"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"hundreds of strawberries and a number of peaches\nthousands of oranges\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Accessing Web Pages","metadata":{}},{"cell_type":"code","source":"from pattern.web import download\n\npage_html = download('https://en.wikipedia.org/wiki/Artificial_intelligence', unicode=True)","metadata":{"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#You can also download files from webpages, for example, images using the URL method","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pattern.web import URL, extension\n\npage_url = URL('https://upload.wikimedia.org/wikipedia/commons/f/f1/RougeOr_football.jpg')\nfile = open('football' + extension(page_url.page), 'wb')\nfile.write(page_url.download())\nfile.close()","metadata":{"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"Finding URLs within Text","metadata":{}},{"cell_type":"code","source":"from pattern.web import find_urls\n\nprint(find_urls('To search anything, go to www.google.com', unique=True))","metadata":{"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"['www.google.com']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Making Asynchronous Requests for Webpages","metadata":{}},{"cell_type":"code","source":"from pattern.web import asynchronous, time, Google\n\nasyn_req = asynchronous(Google().search, 'artificial intelligence', timeout=4)\nwhile not asyn_req.done:\n    time.sleep(0.1)\n    print('searching...')\n\nprint(asyn_req.value)\n\nprint(find_urls(asyn_req.value, unique=True))","metadata":{"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"searching...\nsearching...\nsearching...\nsearching...\nsearching...\nsearching...\nsearching...\nsearching...\nsearching...\nsearching...\n[Result({'url': 'https://en.wikipedia.org/wiki/Artificial_intelligence', 'title': 'Artificial intelligence - Wikipedia', 'text': '<b>Artificial intelligence</b> (<b>AI</b>) is intelligence demonstrated by machines, unlike the <br>\\nnatural intelligence displayed by humans and animals, which involves&nbsp;...'}), Result({'url': 'https://www.investopedia.com/terms/a/artificial-intelligence-ai.asp', 'title': 'Artificial Intelligence (AI) Definition', 'text': '<b>Artificial intelligence</b> (<b>AI</b>) refers to the simulation of human intelligence in <br>\\nmachines that are programmed to think like humans and mimic their actions. The <br>\\nterm&nbsp;...'}), Result({'url': 'https://builtin.com/artificial-intelligence', 'title': 'What is Artificial Intelligence? How Does AI Work? | Built In', 'text': '<b>Artificial intelligence</b> (<b>AI</b>) is wide-ranging branch of computer science concerned <br>\\nwith building smart machines capable of performing tasks that typically require&nbsp;...'}), Result({'url': 'https://futureoflife.org/background/benefits-risks-of-artificial-intelligence/', 'title': 'Benefits & Risks of Artificial Intelligence - Future of Life Institute', 'text': '<b>Artificial intelligence</b> today is properly known as narrow <b>AI</b> (or weak <b>AI</b>), in that it is <br>\\ndesigned to perform a narrow task (e.g. only facial recognition or only internet&nbsp;...'}), Result({'url': 'https://www.brookings.edu/research/what-is-artificial-intelligence/', 'title': 'What is artificial intelligence?', 'text': '... <b>Artificial intelligence</b> algorithms are designed to make decisions, often using real-<br>\\ntime data. They are unlike passive machines that are capable&nbsp;...', 'date': 'Oct 4, 2018'}), Result({'url': 'https://www.britannica.com/technology/artificial-intelligence', 'title': 'artificial intelligence | Definition, Examples, and Applications ...', 'text': '<b>Artificial intelligence</b>, the ability of a computer or computer-controlled robot to <br>\\nperform tasks commonly associated with intelligent beings. The term is frequently<br>\\n&nbsp;...'}), Result({'url': 'https://www.aaai.org/', 'title': 'Association for the Advancement of Artificial Intelligence', 'text': 'AAAI advances the understanding of the mechanisms underlying thought and <br>\\n<b>intelligent</b> behavior and their embodiment in machines.'}), Result({'url': 'https://www.journals.elsevier.com/artificial-intelligence', 'title': 'Artificial Intelligence - Journal - Elsevier', 'text': 'The journal of <b>Artificial Intelligence</b> (AIJ) welcomes papers on broad aspects of <b>AI</b> <br>\\nthat constitute advances in the overall field including, but not...'}), Result({'url': 'https://www.zdnet.com/article/what-is-ai-everything-you-need-to-know-about-artificial-intelligence/', 'title': 'What is AI? Everything you need to know about Artificial Intelligence ...', 'text': '... What is <b>artificial intelligence</b> (<b>AI</b>)?. It depends who you ask. Back in the 1950s, the <br>\\nfathers of the field, Minsky and McCarthy, described artificial&nbsp;...', 'date': 'Dec 11, 2020'}), Result({'url': 'https://www.ibm.com/cloud/learn/what-is-artificial-intelligence', 'title': 'What is Artificial Intelligence (AI)? | IBM', 'text': '... What is <b>artificial intelligence</b>? In computer science, the term <b>artificial intelligence</b> (<br>\\n<b>AI</b>) refers to any human-like intelligence exhibited by a computer&nbsp;...', 'date': 'Jun 3, 2020'})]\n['https://en.wikipedia.org/wiki/Artificial_intelligence', 'https://www.investopedia.com/terms/a/artificial-intelligence-ai.asp', 'https://builtin.com/artificial-intelligence', 'https://futureoflife.org/background/benefits-risks-of-artificial-intelligence/', 'https://www.brookings.edu/research/what-is-artificial-intelligence/', 'https://www.britannica.com/technology/artificial-intelligence', 'https://www.aaai.org/', 'https://www.journals.elsevier.com/artificial-intelligence', 'https://www.zdnet.com/article/what-is-ai-everything-you-need-to-know-about-artificial-intelligence/', 'https://www.ibm.com/cloud/learn/what-is-artificial-intelligence']\n","output_type":"stream"}]},{"cell_type":"code","source":"  we retrieve the Google search result of page 1 for the search query \"artificial intelligence\", \n  you can see that while the page downloads we execute a while loop in parallel. \n  Finally, the results retrieved by the query are printed using the value attribute of the object returned by the asynchronous module.\n  Next, we extract the URLs from the search, which are then printed on the screen.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Getting Search Engine Results with APIs","metadata":{}},{"cell_type":"code","source":"from pattern.web import Google\n\ngoogle = Google(license=None)\nfor search_result in google.search('artificial intelligence'):\n    print(search_result.url)\n    print(search_result.text)","metadata":{"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"https://en.wikipedia.org/wiki/Artificial_intelligence\n<b>Artificial intelligence</b> (<b>AI</b>) is intelligence demonstrated by machines, unlike the <br>\nnatural intelligence displayed by humans and animals, which involves&nbsp;...\nhttps://www.investopedia.com/terms/a/artificial-intelligence-ai.asp\n<b>Artificial intelligence</b> (<b>AI</b>) refers to the simulation of human intelligence in <br>\nmachines that are programmed to think like humans and mimic their actions. The <br>\nterm&nbsp;...\nhttps://builtin.com/artificial-intelligence\n<b>Artificial intelligence</b> (<b>AI</b>) is wide-ranging branch of computer science concerned <br>\nwith building smart machines capable of performing tasks that typically require&nbsp;...\nhttps://futureoflife.org/background/benefits-risks-of-artificial-intelligence/\n<b>Artificial intelligence</b> today is properly known as narrow <b>AI</b> (or weak <b>AI</b>), in that it is <br>\ndesigned to perform a narrow task (e.g. only facial recognition or only internet&nbsp;...\nhttps://www.brookings.edu/research/what-is-artificial-intelligence/\n... <b>Artificial intelligence</b> algorithms are designed to make decisions, often using real-<br>\ntime data. They are unlike passive machines that are capable&nbsp;...\nhttps://www.britannica.com/technology/artificial-intelligence\n<b>Artificial intelligence</b>, the ability of a computer or computer-controlled robot to <br>\nperform tasks commonly associated with intelligent beings. The term is frequently<br>\n&nbsp;...\nhttps://www.aaai.org/\nAAAI advances the understanding of the mechanisms underlying thought and <br>\n<b>intelligent</b> behavior and their embodiment in machines.\nhttps://www.journals.elsevier.com/artificial-intelligence\nThe journal of <b>Artificial Intelligence</b> (AIJ) welcomes papers on broad aspects of <b>AI</b> <br>\nthat constitute advances in the overall field including, but not...\nhttps://www.zdnet.com/article/what-is-ai-everything-you-need-to-know-about-artificial-intelligence/\n... What is <b>artificial intelligence</b> (<b>AI</b>)?. It depends who you ask. Back in the 1950s, the <br>\nfathers of the field, Minsky and McCarthy, described artificial&nbsp;...\nhttps://www.ibm.com/cloud/learn/what-is-artificial-intelligence\n... What is <b>artificial intelligence</b>? In computer science, the term <b>artificial intelligence</b> (<br>\n<b>AI</b>) refers to any human-like intelligence exhibited by a computer&nbsp;...\n","output_type":"stream"}]},{"cell_type":"code","source":"we create an object of Google class. In the constructor of Google, pass your own license key to the license parameter. Next, we pass the string artificial intelligence to the search method. By default, the first 10 results from the first page will be returned which are then iterated, and the url and text of each result is displayed on the screen.\n\nThe process is similar for Bing search engine, you only have to replace the Bing class with Google in the script above.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Twitter for the three latest tweets that contain the text \"artificial intelligence\".","metadata":{}},{"cell_type":"code","source":"from pattern.web import Twitter\n\ntwitter = Twitter()\nindex = None\nfor j in range(3):\n    for tweet in twitter.search('artificial intelligence', start=index, count=3):\n        print(tweet.text)\n        index = tweet.id","metadata":{"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"RT @IainLJBrown: Using Artificial Intelligence To Discover New Drug Treatments: Science Next - WRKF\n\nRead more here: https://t.co/v03TaOrJxF\n\n#ArtificialIntelligence #AI #DataScience #100DaysOfCode #Python #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT\nRT @IainLJBrown: Sonoma County to Use Artificial Intelligence to Detect Wildfires - NBC Bay Area\n\nRead more here: https://t.co/W0zHzWaUoY\n\n#ArtificialIntelligence #AI #DataScience #100DaysOfCode #Python #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT\nRT @IainLJBrown: Sonoma County to Use Artificial Intelligence to Detect Wildfires - NBC Bay Area\n\nRead more here: https://t.co/W0zHzWaUoY\n\n#ArtificialIntelligence #AI #DataScience #100DaysOfCode #Python #MachineLearning #BigData #DeepLearning #NLP #Robots #IoT\nRT @MIT: Spencer Compton, Karna Morey, Tara Venkatadri, and Lily Zhang named 2021-22 Goldwater Scholars: Four MIT undergraduates whose research areas explore artificial intelligence, space, and climate change honored for their academic achievements. https://t.co/12aHdJ8VMw https://t.co/wrhjHdpJaF\nRT @AINewsFeed: Artificial Intelligence: Technologies, Applications, and Challenges -  https://t.co/1bIn3NQJqS #ai #ml #dl\nRT @College_Experts: Colleges Start New Programs: #Nursing , Quantum Science, Extended Reality, Artificial Intelligence, Mental Health, #Optics and #Photonics , Event Design https://t.co/cTat5FKVud #college #collegetalk #ArtificialIntelligence #MentalHealth #QuantumScience #ExtendedReality\nRT @machinelearnflx: The CPSC Digs In on Artificial Intelligence https://t.co/hdlovdMWvp  #ArtificialIntelligence\nRT @nordicinst: JG Wentworth Welcomes Andrey Zelenovsky as their Vice President of Artificial Intelligence and .... #aistrategy #datascience #aiforgood https://t.co/vt5o5oBOBQ\nRT @FENews: #Artificialintelligence will be more revolutionary for education than the internet.\n@Aftab_Hussain from @BoltonCollege\nhttps://t.co/c9j7uLZlhu\n","output_type":"stream"}]},{"cell_type":"code","source":" we first import the Twitter class from the pattern.web module. Next, We iterate over the tweets returned by the Twitter class and display the text of the tweet on the console.\n  You do not need any license key to run the above script.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Converting HTML Data to Plain Text","metadata":{}},{"cell_type":"code","source":"from pattern.web import URL, plaintext\n\nhtml_content = URL('https://stackabuse.com/python-for-nlp-introduction-to-the-textblob-library/').download()\ncleaned_page = plaintext(html_content.decode('utf-8'))\nprint(cleaned_page)","metadata":{"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Python for NLP: Introduction to the TextBlob Library\n\nToggle navigation Stack Abuse\n\n* JavaScript\n* Python\n* Java\n* Jobs\n\nPython for NLP: Introduction to the TextBlob Library\n\nBy\nUsman Malik\n•0 Comments\n\nIntroduction\n\nThis is the seventh article in my series of articles on Python for NLP. In my previous article, I explained how to perform topic modeling using Latent Dirichlet Allocation and Non-Negative Matrix factorization. We used the Scikit-Learn library to perform topic modeling.\n\nIn this article, we will explore TextBlob, which is another extremely powerful NLP library for Python. TextBlob is built upon NLTK and provides an easy to use interface to the NLTK library. We will see how TextBlob can be used to perform a variety of NLP tasks ranging from parts-of-speech tagging to sentiment analysis, and language translation to text classification.\n\nThe detailed download instructions for the library can be found at the official link. I would suggest that you install the TextBlob library as well as the sample corpora.\n\nHere is the gist of the instructions linked above, but be sure to check the official documentation for more instructions on installing if you need it:\n\n$ pip install -U textblob\n\nAnd to install the corpora:\n\n$ python -m textblob.download_corpora\n\nLet's now see the different functionalities of the TextBlob library.\n\nTokenization\n\nTokenization refers to splitting a large paragraph into sentences or words. Typically, a token refers to a word in a text document. Tokenization is pretty straight forward with TextBlob. All you have to do is import the\nTextBlob\n\nobject from the\ntextblob\n\nlibrary, pass it the document that you want to tokenize, and then use the\nsentences\n\nand\nwords\n\nattributes to get the tokenized sentences and attributes. Let's see this in action:\n\nThe first step is to import the\nTextBlob\n\nobject:\n\nfrom textblob import TextBlob\n\nNext, you need to define a string that contains the text of the document. We will create string that contains the first paragraph of the Wikipedia article on artificial intelligence.\n\ndocument = (\"In computer science, artificial intelligence (AI), \\\nsometimes called machine intelligence, is intelligence \\\ndemonstrated by machines, in contrast to the natural intelligence \\\ndisplayed by humans and animals. Computer science defines AI \\\nresearch as the study of \\\"intelligent agents\\\": any device that \\\nperceives its environment and takes actions that maximize its\\\nchance of successfully achieving its goals.[1] Colloquially,\\\nthe term \\\"artificial intelligence\\\" is used to describe machines\\\nthat mimic \\\"cognitive\\\" functions that humans associate with other\\\nhuman minds, such as \\\"learning\\\" and \\\"problem solving\\\".[2]\")\n\nThe next step is to pass this document as a parameter to the\nTextBlob\n\nclass. The returned object can then be used to tokenize the document to words and sentences.\n\ntext_blob_object = TextBlob(document)\n\nNow to get the tokenized sentences, we can use the\nsentences\n\nattribute:\n\ndocument_sentence = text_blob_object.sentences\n\nprint(document_sentence)\nprint(len(document_sentence))\n\nIn the output, you will see the tokenized sentences along with the number of sentences.\n\n[Sentence(\"In computer science, artificial intelligence (AI), sometimes called machine intelligence, is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and animals.\"), Sentence(\"Computer science defines AI research as the study of \"intelligent agents\": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals.\"), Sentence(\"[1] Colloquially, the term \"artificial intelligence\" is used to describe machines that mimic \"cognitive\" functions that humans associate with other human minds, such as \"learning\" and \"problem solving\".\"), Sentence(\"[2]\")]\n4\n\nSimilarly, the\nwords\n\nattribute returns the tokenized words in the document.\n\ndocument_words = text_blob_object.words\n\nprint(document_words)\nprint(len(document_words))\n\nThe output looks like this:\n\n['In', 'computer', 'science', 'artificial', 'intelligence', 'AI', 'sometimes', 'called', 'machine', 'intelligence', 'is', 'intelligence', 'demonstrated', 'by', 'machines', 'in', 'contrast', 'to', 'the', 'natural', 'intelligence', 'displayed', 'by', 'humans', 'and', 'animals', 'Computer', 'science', 'defines', 'AI', 'research', 'as', 'the', 'study', 'of', 'intelligent', 'agents', 'any', 'device', 'that', 'perceives', 'its', 'environment', 'and', 'takes', 'actions', 'that', 'maximize', 'its', 'chance', 'of', 'successfully', 'achieving', 'its', 'goals', '1', 'Colloquially', 'the', 'term', 'artificial', 'intelligence', 'is', 'used', 'to', 'describe', 'machines', 'that', 'mimic', 'cognitive', 'functions', 'that', 'humans', 'associate', 'with', 'other', 'human', 'minds', 'such', 'as', 'learning', 'and', 'problem', 'solving', '2']\n84\n\nLemmatization\n\nLemmatization refers to reducing the word to its root form as found in a dictionary.\n\nTo perform lemmatization via TextBlob, you have to use the\nWord\n\nobject from the\ntextblob\n\nlibrary, pass it the word that you want to lemmatize and then call the\nlemmatize\n\nmethod.\n\nfrom textblob import Word\n\nword1 = Word(\"apples\")\nprint(\"apples:\", word1.lemmatize())\n\nword2 = Word(\"media\")\nprint(\"media:\", word2.lemmatize())\n\nword3 = Word(\"greater\")\nprint(\"greater:\", word3.lemmatize(\"a\"))\n\nIn the script above, we perform lemmatization on the words \"apples\", \"media\", and \"greater\". In the output, you will see the words \"apple\", (which is singular for the apple), \"medium\" (which is singular for the medium) and \"great\" (which is the positive degree for the word greater). Notice that for the word greater, we pass \"a\" as a parameter to the\nlemmatize\n\nmethod. This specifically tells the method that the word should be treated as an adjective. By default, the words are treated as nouns by the\nlemmatize()\n\nmethod. The complete list for the parts of speech components is as follows:\n\nADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'\n\nParts of Speech (POS) Tagging\n\nLike the spaCy and NLTK libraries, the TextBlob library also contains functionalities for the POS tagging.\n\nTo find POS tags for the words in a document, all you have to do is use the\ntags\n\nattribute as shown below:\n\nfor word, pos in text_blob_object.tags:\nprint(word + \" => \" + pos)\n\nIn the script above, print the tags for all the words in the first paragraph of the Wikipedia article on Artificial Intelligence. The output of the script above looks like this:\n\n```\nIn => IN\ncomputer => NN\nscience => NN\nartificial => JJ\nintelligence => NN\nAI => NNP\nsometimes => RB\ncalled => VBD\nmachine => NN\nintelligence => NN\nis => VBZ\nintelligence => NN\ndemonstrated => VBN\nby => IN\nmachines => NNS\nin => IN\ncontrast => NN\nto => TO\nthe => DT\nnatural => JJ\nintelligence => NN\ndisplayed => VBN\nby => IN\nhumans => NNS\nand => CC\nanimals => NNS\nComputer => NNP\nscience => NN\ndefines => NNS\nAI => NNP\nresearch => NN\nas => IN\nthe => DT\nstudy => NN\nof => IN\nintelligent => JJ\nagents => NNS\nany => DT\ndevice => NN\nthat => WDT\nperceives => VBZ\nits => PRP$\nenvironment => NN\nand => CC\ntakes => VBZ\nactions => NNS\nthat => IN\nmaximize => VB\nits => PRP$\nchance => NN\nof => IN\nsuccessfully => RB\nachieving => VBG\nits => PRP$\ngoals => NNS\n[ => RB\n1 => CD\n] => NNP\nColloquially => NNP\nthe => DT\nterm => NN\nartificial => JJ\nintelligence => NN\nis => VBZ\nused => VBN\nto => TO\ndescribe => VB\nmachines => NNS\nthat => IN\nmimic => JJ\ncognitive => JJ\nfunctions => NNS\nthat => WDT\nhumans => NNS\nassociate => VBP\nwith => IN\nother => JJ\nhuman => JJ\nminds => NNS\nsuch => JJ\nas => IN\nlearning => VBG\nand => CC\nproblem => NN\nsolving => NN\n[ => RB\n2 => CD\n] => NNS\n```\n\nThe POS tags have been printed in the abbreviation form. To see the full form of each abbreviation, please consult this link.\n\nConvert Text to Singular and Plural\n\nTextBlob also allows you to convert text into a plural or singular form using the\npluralize\n\nand\nsingularize\n\nmethods, respectively. Look at the following example:\n\ntext = (\"Football is a good game. It has many health benefit\")\ntext_blob_object = TextBlob(text)\nprint(text_blob_object.words.pluralize())\n\nIn the output, you will see the plural of all the words:\n\n['Footballs', 'iss', 'some', 'goods', 'games', 'Its', 'hass', 'manies', 'healths', 'benefits']\n\nSimilarly, to singularize words you can use\nsingularize\n\nmethod as follows:\n\ntext = (\"Footballs is a goods games. Its has many healths benefits\")\n\ntext_blob_object = TextBlob(text)\nprint(text_blob_object.words.singularize())\n\nThe output of the script above looks like this:\n\n['Football', 'is', 'a', 'good', 'game', 'It', 'ha', 'many', 'health', 'benefit']\n\nNoun Phrase Extraction\n\nNoun phrase extraction, as the name suggests, refers to extracting phrases that contain nouns. Let's find all the noun phrases in the first paragraph of the Wikipedia article on artificial intelligence that we used earlier.\n\nTo find noun phrases, you simply have to use the\nnoun_phrase\n\nattributes on the\nTextBlob\n\nobject. Look at the following example:\n\ntext_blob_object = TextBlob(document)\nfor noun_phrase in text_blob_object.noun_phrases:\nprint(noun_phrase)\n\nThe output looks like this:\n\ncomputer science\nartificial intelligence\nai\nmachine intelligence\nnatural intelligence\ncomputer\nscience defines\nai\nintelligent agents\ncolloquially\nartificial intelligence\ndescribe machines\nhuman minds\n\nYou can see all the noun phrases in our document.\n\nGetting Words and Phrase Counts\n\nIn a previous section, we used Python's built-in\nlen\n\nmethod to count the number of sentences, words and noun-phrases returned by the\nTextBlob\n\nobject. We can use TextBlob's built-in methods for the same purpose.\n\nTo find the frequency of occurrence of a particular word, we have to pass the name of the word as the index to the\nword_counts\n\nlist of the\nTextBlob\n\nobject.\n\nIn the following example, we will count the number of instances of the word \"intelligence\" in the first paragraph of the Wikipedia article on Artificial Intelligence.\n\ntext_blob_object = TextBlob(document)\ntext_blob_object.word_counts['intelligence']\n\nAnother way is to simply call the\ncount\n\nmethod on the\nwords\n\nattribute, and pass the name of the word whose frequency of occurrence is to be found as shown below:\n\ntext_blob_object.words.count('intelligence')\n\nIt is important to mention that by default the search is not case-sensitive. If you want your search to be case sensitive, you need to pass\nTrue\n\nas the value for the\ncase_sensitive\n\nparameter, as shown below:\n\ntext_blob_object.words.count('intelligence', case_sensitive=True)\n\nLike word counts, noun phrases can also be counted in the same way. The following example finds the phrase \"artificial intelligence\" in the paragraph.\n\ntext_blob_object = TextBlob(document)\ntext_blob_object.noun_phrases.count('artificial intelligence')\n\nIn the output, you will see 2.\n\nConverting to Upper and Lowercase\n\nTextBlob objects are very similar to strings. You can convert them to upper case or lower case, change their values, and concatenate them together as well. In the following script, we convert the text from the TextBlob object to upper case:\n\ntext = \"I love to watch football, but I have never played it\"\ntext_blob_object = TextBlob(text)\n\nprint(text_blob_object.upper())\n\nIn the output, you will the string in the upper case:\n\nI LOVE TO WATCH FOOTBALL, BUT I HAVE NEVER PLAYED IT\n\nSimilarly to convert the text to lowercase, we can use the\nlower()\n\nmethod as shown below:\n\ntext = \"I LOVE TO WATCH FOOTBALL, BUT I HAVE NEVER PLAYED IT\"\ntext_blob_object = TextBlob(text)\n\nprint(text_blob_object.lower())\n\nFinding N-Grams\n\nN-Grams refer to n combination of words in a sentence. For instance, for a sentence \"I love watching football\", some 2-grams would be (I love), (love watching) and (watching football). N-Grams can play a crucial role in text classification.\n\nIn TextBlob, N-grams can be found by passing the number of N-Grams to the\nngrams\n\nmethod of the\nTextBlob\n\nobject. Look at the following example:\n\ntext = \"I love to watch football, but I have never played it\"\ntext_blob_object = TextBlob(text)\nfor ngram in text_blob_object.ngrams(2):\nprint(ngram)\n\nThe output of the script looks like this:\n\n['I', 'love']\n['love', 'to']\n['to', 'watch']\n['watch', 'football']\n['football', 'but']\n['but', 'I']\n['I', 'have']\n['have', 'never']\n['never', 'played']\n['played', 'it']\n\nThis is especially helpful when training language models or doing any type of text prediction.\n\nSpelling Corrections\n\nSpelling correction is one of the unique functionalities of the TextBlob library. With the\ncorrect\n\nmethod of the\nTextBlob\n\nobject, you can correct all the spelling mistakes in your text. Look at the following example:\n\ntext = \"I love to watchf footbal, but I have neter played it\"\ntext_blob_object = TextBlob(text)\n\nprint(text_blob_object.correct())\n\nIn the script above we made three spelling mistakes: \"watchf\" instead of \"watch\", \"footbal\" instead of \"football\", \"neter\" instead of \"never\". In the output, you will see that these mistakes have been corrected by TextBlob, as shown below:\n\nI love to watch football, but I have never played it\n\nLanguage Translation\n\nOne of the most powerful capabilities of the TextBlob library is to translate from one language to another. On the backend, the TextBlob language translator uses the Google Translate API\n\nTo translate from one language to another, you simply have to pass the text to the\nTextBlob\n\nobject and then call the\ntranslate\n\nmethod on the object. The language code for the language that you want your text to be translated to is passed as a parameter to the method. Let's take a look at an example:\n\ntext_blob_object_french = TextBlob(u'Salut comment allez-vous?')\nprint(text_blob_object_french.translate(to='en'))\n\nIn the script above, we pass a sentence in the French language to the\nTextBlob\n\nobject. Next, we call the\ntranslate\n\nmethod on the object and pass language code\nen\n\nto the\nto\n\nparameter. The language code\nen\n\ncorresponds to the English language. In the output, you will see the translation of the French sentence as shown below:\n\nHi, how are you?\n\nLet's take another example where we will translate from Arabic to English:\n\ntext_blob_object_arabic = TextBlob(u'مرحبا كيف حالك؟')\nprint(text_blob_object_arabic.translate(to='en'))\n\nOutput:\n\nHi, how are you?\n\nFinally, using the\ndetect_language\n\nmethod, you can also detect the language of the sentence. Look at the following script:\n\ntext_blob_object = TextBlob(u'Hola como estas?')\nprint(text_blob_object.detect_language())\n\nIn the output, you will see\nes\n\n, which stands for the Spanish language.\n\nThe language code for all the languages can be found at this link.\n\nText Classification\n\nTextBlob also provides basic text classification capabilities. Though, I would not recommend TextBlob for text classification owing to its limited capabilities, however, if you have a really limited data and you want to quickly develop a very basic text classification model, then you may use TextBlob. For advanced models, I would recommend machine learning libraries such as Scikit-Learn or Tensorflow.\n\nLet's see how we can perform text classification with TextBlob. The first thing we need is a training dataset and test data. The classification model will be trained on the training dataset and will be evaluated on the test dataset.\n\nSuppose we have the following training and test data:\n\ntrain_data = [\n('This is an excellent movie', 'pos'),\n('The move was fantastic I like it', 'pos'),\n('You should watch it, it is brilliant', 'pos'),\n('Exceptionally good', 'pos'),\n(\"Wonderfully directed and executed. I like it\", 'pos'),\n('It was very boring', 'neg'),\n('I did not like the movie', 'neg'),\n(\"The movie was horrible\", 'neg'),\n('I will not recommend', 'neg'),\n('The acting is pathetic', 'neg')\n]\ntest_data = [\n('Its a fantastic series', 'pos'),\n('Never watched such a brillent movie', 'pos'),\n(\"horrible acting\", 'neg'),\n(\"It is a Wonderful movie\", 'pos'),\n('waste of money', 'neg'),\n(\"pathetic picture\", 'neg')\n]\n\nThe dataset contains some dummy reviews about movies. You can see our training and test datasets consist of lists of tuples where the first element of the tuple is the text or a sentence while the second member of the tuple is the corresponding review or sentiment of the text.\n\nWe will train our dataset on the\ntrain_data\n\nand will evaluate it on the\ntest_data\n\n. To do so, we will use the\nNaiveBayesClassifier\n\nclass from the\ntextblob.classifiers\n\nlibrary. The following script imports the library:\n\nfrom textblob.classifiers import NaiveBayesClassifier\n\nTo train the model, we simply have to pass the training data to the constructor of the\nNaiveBayesClassifier\n\nclass. The class will return an object trained on the dataset and capable of making predictions on the test set.\n\nclassifier = NaiveBayesClassifier(train_data)\n\nLet's first make a prediction on a single sentence. To do so, we need to call the\nclassify\n\nmethod and pass it the sentence. Look at the following example:\n\nprint(classifier.classify(\"It is very boring\"))\n\nIt looks like a negative review. When you execute the above script, you will see\nneg\n\nin the output.\n\nSimilarly, the following script will return\npos\n\nsince the review is positive.\n\nprint(classifier.classify(\"It's a fantastic series\"))\n\nYou can also make a prediction by passing our\nclassifier\n\nto the\nclassifier\n\nparameter of the\nTextBlob\n\nobject. You then have to call the\nclassify\n\nmethod on the\nTextBlob\n\nobject to view the prediction.\n\nsentence = TextBlob(\"It's a fantastic series.\", classifier=classifier)\nprint(sentence.classify())\n\nFinally, to find the accuracy of your algorithm on the test set, call the\naccuracy\n\nmethod on your classifier and pass it the\ntest_data\n\nthat we just created. Look at the following script:\n\nclassifier.accuracy(test_data)\n\nIn the output, you will see 0.66 which is the accuracy of the algorithm.\n\nTo find the most important features for the classification, the\nshow_informative_features\n\nmethod can be used. The number of most important features to see is passed as a parameter.\n\nclassifier.show_informative_features(3)\n\nThe output looks like this:\n\nMost Informative Features\ncontains(it) = False neg : pos = 2.2 : 1.0\ncontains(is) = True pos : neg = 1.7 : 1.0\ncontains(was) = True neg : pos = 1.7 : 1.0\n\nIn this section, we tried to find the sentiment of the movie review using text classification. In reality, you don't have to perform text classification to find the sentiment of a sentence in TextBlob. The TextBlob library comes with a built-in sentiment analyzer which we will see in the next section.\n\nSentiment Analysis\n\nIn this section, we will analyze the sentiment of the public reviews for different foods purchased via Amazon. We will use the TextBlob sentiment analyzer to do so.\n\nThe dataset can be downloaded from this Kaggle link.\n\nAs a first step, we need to import the dataset. We will only import the first 20,000 records due to memory constraints. You can import more records if you want. The following script imports the dataset:\n\nimport pandas as pd\nimport numpy as np\n\nreviews_datasets = pd.read_csv(r'E:\\Datasets\\Reviews.csv')\nreviews_datasets = reviews_datasets.head(20000)\nreviews_datasets.dropna()\n\nTo see how our dataset looks, we will use the\nhead\n\nmethod of the pandas data frame:\n\nreviews_datasets.head()\n\nThe output looks like this:\n\nFrom the output, you can see that the text review about the food is contained by the Text column. The score column contains ratings of the user for the particular product with 1 being the lowest and 5 being the highest rating.\n\nLet's see the distribution of rating:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.distplot(reviews_datasets['Score'])\n\nYou can see that most of the ratings are highly positive i.e. 5. Let's plot the bar plot for the ratings to have a better look at the number of records for each rating.\n\nsns.countplot(x='Score', data=reviews_datasets)\n\nThe output shows that more than half of reviews have 5-star ratings.\n\nLet's randomly select a review and find its polarity using TextBlob. Let's take a look at review number 350.\n\nreviews_datasets['Text'][350]\n\nOutput:\n\n'These chocolate covered espresso beans are wonderful! The chocolate is very dark and rich and the \"bean\" inside is a very delightful blend of flavors with just enough caffine to really give it a zing.'\n\nIt looks like the review is positive. Let's verify this using the TextBlob library. To find the sentiment, we have to use the\nsentiment\n\nattribute of the\nTextBlog\n\nobject. The\nsentiment\n\nobject returns a tuple that contains polarity and subjectivity of the review.\n\nThe value of polarity can be between -1 and 1 where the reviews with negative polarities have negative sentiments while the reviews with positive polarities have positive sentiments.\n\nThe subjectivity value can be between 0 and 1. Subjectivity quantifies the amount of personal opinion and factual information contained in the text. The higher subjectivity means that the text contains personal opinion rather than factual information.\n\nLet's find the sentiment of the 350th review.\n\ntext_blob_object = TextBlob(reviews_datasets['Text'][350])\nprint(text_blob_object.sentiment)\n\nThe output looks like this:\n\nSentiment(polarity=0.39666666666666667,subjectivity=0.6616666666666667)\n\nThe output shows that the review is positive with a high subjectivity.\n\nLet's now add a column for sentiment polarity in our dataset. Execute the following script:\n\ndef find_pol(review):\nreturn TextBlob(review).sentiment.polarity\n\nreviews_datasets['Sentiment_Polarity'] = reviews_datasets['Text'].apply(find_pol)\nreviews_datasets.head()\n\nNow let's see the distribution of polarity in our dataset. Execute the following script:\n\nsns.distplot(reviews_datasets['Sentiment_Polarity'])\n\nThe output of the script above looks like this:\n\nIt is evident from the figure above that most of the reviews are positive and have polarity between 0 and 0.5. This is natural since most of the reviews in the dataset have 5-star ratings.\n\nLet's now plot the average polarity for each score rating.\n\nsns.barplot(x='Score', y='Sentiment_Polarity', data=reviews_datasets)\n\nOutput:\n\nThe output clearly shows that the reviews with high rating scores have high positive polarities.\n\nLet's now see some of the most negative reviews i.e. the reviews with a polarity value of -1.\n\nmost_negative = reviews_datasets[reviews_datasets.Sentiment_Polarity == -1].Text.head()\nprint(most_negative)\n\nThe output looks like this:\n\n545 These chips are nasty. I thought someone had ...\n1083 All my fault. I thought this would be a carton...\n1832 Pop Chips are basically a horribly over-priced...\n2087 I do not consider Gingerbread, Spicy Eggnog, C...\n2763 This popcorn has alot of hulls I order 4 bags ...\nName: Text, dtype: object\n\nLet's print the value of review number 545.\n\nreviews_datasets['Text'][545]\n\nIn the output, you will see the following review:\n\n'These chips are nasty. I thought someone had spilled a drink in the bag, no the chips were just soaked with grease. Nasty!!'\n\nThe output clearly shows that the review is highly negative.\n\nLet's now see some of the most positive reviews. Execute the following script:\n\nmost_positive = reviews_datasets[reviews_datasets.Sentiment_Polarity == 1].Text.head()\nprint(most_positive)\n\nThe output looks like this:\n\n106 not what I was expecting in terms of the compa...\n223 This is an excellent tea. One of the best I h...\n338 I like a lot of sesame oil and use it in salad...\n796 My mother and father were the recipient of the...\n1031 The Kelloggs Muselix are delicious and the del...\nName: Text, dtype: object\n\nLet's see review 106 in detail:\n\nreviews_datasets['Text'][106]\n\nOutput:\n\n\"not what I was expecting in terms of the company's reputation for excellent home delivery products\"\n\nYou can see that though the review was not very positive, it has been assigned a polarity of 1 due to the presence of words like\nexcellent\n\nand\nreputation\n\n. It is important to know that sentiment analyzer is not 100% error-proof and might predict wrong sentiment in a few cases, such as the one we just saw.\n\nLet's now see review number 223 which also has been marked as positive.\n\nreviews_datasets['Text'][223]\n\nThe output looks like this:\n\n\"This is an excellent tea. One of the best I have ever had. It is especially great when you prepare it with a samovar.\"\n\nThe output clearly depicts that the review is highly positive.\n\nConclusion\n\nPython's TextBlob library is one of the most famous and widely used natural language processing libraries. This article explains several functionalities of the TextBlob library, such as tokenization, stemming, sentiment analysis, text classification and language translation in detail.\n\nIn the next article I'll go over the Pattern library, which provides a lot of really useful functions for determining attributes about sentences, as well as tools for retrieving data from social networks, Wikipedia, and search engines.\n\npython,nlp\n\n*\n*\n*\n\nAbout Usman Malik\n\nParis (France) Twitter\n\nProgrammer | Blogger | Data Science Enthusiast | PhD To Be | Arsenal FC for Life\n\nSubscribe to our Newsletter\n\nPlease enable JavaScript to view the comments powered by Disqus.\n\nPrevious Post\nNext Post\n\nAd\n\nreport this ad\n\nFollow Us\n\nTwitter\n\nFacebook\n\nRSS\n\nData Vizualization in Python\n\nUnderstand your data better with visualizations!\n\n* ✅ 30-day no-questions money-back guarantee\n* ✅ Beginner to Advanced\n* ✅ Updated regularly (latest update April 2021)\n* ✅ Updated with bonus resources and guides\n\nLearn more\n\nGetting Started with AWS in Node\n\nJust released! Build the foundation you'll need to provision, deploy, and run Node.js applications in the AWS cloud. Learn Lambda, EC2, S3, SQS, and more!\n\nLearn more\n\nGit Essentials\n\nJust released! Check out this hands-on, practical guide to learning Git, with best-practices and industry-accepted standards. Stop Googling Git commands and actually learn it!\n\nLearn more\n\nNewsletter\n\nSubscribe to our newsletter! Get occassional tutorials, guides, and reviews in your inbox.\n\nAd\n\nreport this ad\n\nWant a remote job?\n\nMore jobs\n\nJobs via\nHireRemote.io\n\nPrepping for an interview?\n\n* Improve your skills by solving one coding problem every day\n\n* Get the solutions the next morning via email\n\n* Practice on actual problems asked by top companies, like:\n\nDaily Coding Problem\n\nAd\n\nreport this ad\n\nRecent Posts\n\nCreating a PDF Document in Python with pText\n\nGuide to Understanding chmod\n\nGuide to Using The Django MongoDB Engine with Python\n\nTags\n\naialgorithmsamqpangularannouncementsapacheapache commonsapiarduinoartificial intelligence\n\nFollow Us\n\nTwitter\n\nFacebook\n\nRSS\n\nCopyright © 2021, Stack Abuse. All Rights Reserved.\n\nDisclosure•Privacy Policy•Terms of Service\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Parsing PDF Documments","metadata":{}},{"cell_type":"code","source":"from pattern.web import URL, PDF\n\npdf_doc = URL('http://demo.clab.cs.cmu.edu/NLP/syllabus_f18.pdf').download()\nprint(PDF(pdf_doc.decode('utf-8')))","metadata":{"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/pattern/web/__init__.py\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(self, path, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4382\u001b[0m             \u001b[0minterpreter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPDFPageInterpreter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4383\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4384\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mPDFPage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_pages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxpages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/pattern/web/__init__.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   4338\u001b[0m         \"\"\"\n\u001b[0;32m-> 4339\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4340\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/genericpath.py\u001b[0m in \u001b[0;36mexists\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: embedded null byte","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mPDFError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-beaa2c93c317>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpdf_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mURL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'http://demo.clab.cs.cmu.edu/NLP/syllabus_f18.pdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/pattern/web/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, output)\u001b[0m\n\u001b[1;32m   4366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4367\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4368\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4370\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/pattern/web/__init__.py\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(self, path, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4386\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4387\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4388\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPDFError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4389\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4390\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_utf8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mPDFError\u001b[0m: embedded null byte"],"ename":"PDFError","evalue":"embedded null byte","output_type":"error"}]},{"cell_type":"code","source":"In the script we download a document using the download function. \nNext, the downloaded HTML document is passed to the PDF class which finally prints it on the console.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Clearing the Cache","metadata":{}},{"cell_type":"code","source":"from pattern.web import cache\n\ncache.clear()","metadata":{"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}